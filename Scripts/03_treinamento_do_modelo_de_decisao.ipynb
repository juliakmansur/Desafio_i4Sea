{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configurações Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import joblib\n",
    "\n",
    "from Modules.operation_analiser import OperationLoader, OperationAnalyzer\n",
    "from Modules.forecast_loader import ForecastLoader\n",
    "from Modules.variables_info import variables_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros gerais\n",
    "lat_porto = -25.500511\n",
    "lon_porto = -48.519331\n",
    "forecast_path = '../Database/forecast_data'\n",
    "paradas_path = '../Database/Parada_2024_chuva.xlsx'\n",
    "timebox = ('202405', '202412')\n",
    "\n",
    "variaveis = variables_info['variaveis']\n",
    "legendas = variables_info['legendas']\n",
    "legendas_dict = dict(zip(variaveis, legendas))\n",
    "\n",
    "# Repositorios de Saída\n",
    "base_output = '../Output/Analise_03'\n",
    "os.makedirs('../Output/Analise_03', exist_ok=True)\n",
    "for subfolder in ['Modelos', 'Tables', 'Imagens', 'Imagens/Metricas', 'Imagens/Modelos']:\n",
    "    os.makedirs(os.path.join(base_output, subfolder), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importação dos Dados para Analise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = ForecastLoader(forecast_path, lat_target=lat_porto, lon_target=lon_porto)\n",
    "df_forecast = forecast.load_forecasts(timebox, drop_duplicates=True)\n",
    "\n",
    "operation_loader = OperationLoader(paradas_path)\n",
    "df_paradas = operation_loader.load()\n",
    "\n",
    "analyzer = OperationAnalyzer(df_paradas, df_forecast)\n",
    "df_unificado = analyzer.marcar_paradas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "melhores = pd.read_csv(f'../Output/Analise_02/Tables/melhores_thresholds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento do Modelo Supervisionado para Classificação das Recomendações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define os cenários com base nas métricas\n",
    "cenarios = {\n",
    "    'conservador': melhores.sort_values('recall', ascending=False).head(3),\n",
    "    'moderado': melhores.sort_values('f1_score', ascending=False).head(3),\n",
    "    'arrojado': melhores.sort_values('precision', ascending=False).head(3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CENÁRIO: CONSERVADOR ===\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.79      0.85      1116\n",
      "           1       0.54      0.80      0.64       349\n",
      "\n",
      "    accuracy                           0.79      1465\n",
      "   macro avg       0.73      0.79      0.75      1465\n",
      "weighted avg       0.83      0.79      0.80      1465\n",
      "\n",
      "Matriz de Confusão:  [[877 239]\n",
      " [ 70 279]]\n",
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.87      1116\n",
      "           1       0.58      0.74      0.65       349\n",
      "\n",
      "    accuracy                           0.81      1465\n",
      "   macro avg       0.75      0.79      0.76      1465\n",
      "weighted avg       0.83      0.81      0.82      1465\n",
      "\n",
      "Matriz de Confusão:  [[928 188]\n",
      " [ 89 260]]\n",
      "Modelos salvos: ../Output/Analise_03/Modelos/model_randomforest_conservador.pkl e ../Output/Analise_03/Modelos/model_logisticregression_conservador.pkl\n",
      "\n",
      "=== CENÁRIO: MODERADO ===\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.85      0.88      1116\n",
      "           1       0.61      0.74      0.67       349\n",
      "\n",
      "    accuracy                           0.82      1465\n",
      "   macro avg       0.76      0.79      0.77      1465\n",
      "weighted avg       0.84      0.82      0.83      1465\n",
      "\n",
      "Matriz de Confusão:  [[949 167]\n",
      " [ 91 258]]\n",
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.85      0.88      1116\n",
      "           1       0.61      0.74      0.67       349\n",
      "\n",
      "    accuracy                           0.82      1465\n",
      "   macro avg       0.76      0.79      0.77      1465\n",
      "weighted avg       0.84      0.82      0.83      1465\n",
      "\n",
      "Matriz de Confusão:  [[949 167]\n",
      " [ 91 258]]\n",
      "Modelos salvos: ../Output/Analise_03/Modelos/model_randomforest_moderado.pkl e ../Output/Analise_03/Modelos/model_logisticregression_moderado.pkl\n",
      "\n",
      "=== CENÁRIO: ARROJADO ===\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.88      1116\n",
      "           1       0.85      0.19      0.32       349\n",
      "\n",
      "    accuracy                           0.80      1465\n",
      "   macro avg       0.82      0.59      0.60      1465\n",
      "weighted avg       0.81      0.80      0.75      1465\n",
      "\n",
      "Matriz de Confusão:  [[1104   12]\n",
      " [ 281   68]]\n",
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.88      1116\n",
      "           1       0.85      0.19      0.32       349\n",
      "\n",
      "    accuracy                           0.80      1465\n",
      "   macro avg       0.82      0.59      0.60      1465\n",
      "weighted avg       0.81      0.80      0.75      1465\n",
      "\n",
      "Matriz de Confusão:  [[1104   12]\n",
      " [ 281   68]]\n",
      "Modelos salvos: ../Output/Analise_03/Modelos/model_randomforest_arrojado.pkl e ../Output/Analise_03/Modelos/model_logisticregression_arrojado.pkl\n"
     ]
    }
   ],
   "source": [
    "metricas_modelos = []\n",
    "\n",
    "for nome_cenario, df_regras in cenarios.items():\n",
    "    print(f\"\\n=== CENÁRIO: {nome_cenario.upper()} ===\")\n",
    "\n",
    "    # Gera variáveis binárias com base nos thresholds\n",
    "    for _, row in df_regras.iterrows():\n",
    "        col = row['variavel']\n",
    "        thresh = row['threshold']\n",
    "        df_unificado[f\"{col}_bin\"] = (df_unificado[col] > thresh).astype(int)\n",
    "\n",
    "    variaveis_modelo = [f\"{v}_bin\" for v in df_regras['variavel'].unique().tolist()]\n",
    "    X = df_unificado[variaveis_modelo]\n",
    "    y = df_unificado['parada']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    # Modelos\n",
    "    model_rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "    model_rf.fit(X_train, y_train)\n",
    "\n",
    "    model_lr = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "    model_lr.fit(X_train, y_train)\n",
    "\n",
    "    # Avaliação Random Forest\n",
    "    y_pred_rf = model_rf.predict(X_test)\n",
    "    print(\"Random Forest\")\n",
    "    print(classification_report(y_test, y_pred_rf))\n",
    "    print(\"Matriz de Confusão: \", confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "    report_rf = classification_report(y_test, y_pred_rf, output_dict=True)\n",
    "    for classe in ['0', '1']:\n",
    "        for metric in ['precision', 'recall', 'f1-score']:\n",
    "            metricas_modelos.append({\n",
    "                'cenario': nome_cenario,\n",
    "                'modelo': 'RandomForest',\n",
    "                'classe': classe,\n",
    "                'metric': metric,\n",
    "                'value': report_rf[classe][metric]\n",
    "            })\n",
    "\n",
    "    # Avaliação Regressão Logística\n",
    "    y_pred_lr = model_lr.predict(X_test)\n",
    "    print(\"Logistic Regression\")\n",
    "    print(classification_report(y_test, y_pred_lr))\n",
    "    print(\"Matriz de Confusão: \", confusion_matrix(y_test, y_pred_lr))\n",
    "\n",
    "    report_lr = classification_report(y_test, y_pred_lr, output_dict=True)\n",
    "    for classe in ['0', '1']:\n",
    "        for metric in ['precision', 'recall', 'f1-score']:\n",
    "            metricas_modelos.append({\n",
    "                'cenario': nome_cenario,\n",
    "                'modelo': 'LogisticRegression',\n",
    "                'classe': classe,\n",
    "                'metric': metric,\n",
    "                'value': report_lr[classe][metric]\n",
    "            })\n",
    "\n",
    "    # Salva modelos\n",
    "    modelo_rf_path = f\"{base_output}/Modelos/model_randomforest_{nome_cenario}.pkl\"\n",
    "    modelo_lr_path = f\"{base_output}/Modelos/model_logisticregression_{nome_cenario}.pkl\"\n",
    "    joblib.dump(model_rf, modelo_rf_path)\n",
    "    joblib.dump(model_lr, modelo_lr_path)\n",
    "    print(f\"Modelos salvos: {modelo_rf_path} e {modelo_lr_path}\")\n",
    "\n",
    "    variaveis_originais = [v.replace('_bin', '') for v in variaveis_modelo]\n",
    "    df_vars = pd.DataFrame({\n",
    "        'variavel': variaveis_originais,\n",
    "        'variavel_bin': variaveis_modelo\n",
    "    })\n",
    "    for modelo_nome in ['logisticregression', 'randomforest']:\n",
    "        var_path = f\"{base_output}/Modelos/variaveis_{modelo_nome}_{nome_cenario}.csv\"\n",
    "        df_vars.to_csv(var_path, index=False, header=True)\n",
    "\n",
    "    # Exporta métricas (de todos os cenários juntos)\n",
    "    metricas_df = pd.DataFrame(metricas_modelos)\n",
    "    metricas_df.to_csv(f\"{base_output}/Tables/metricas_modelos_por_cenario.csv\", index=False)\n",
    "\n",
    "    # Importância das variáveis (Random Forest)\n",
    "    importances_rf = pd.Series(model_rf.feature_importances_, index=[legendas_dict.get(v.replace('_bin', ''), v) for v in X.columns])\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    importances_rf.sort_values().plot(kind='barh', ax=ax)\n",
    "    ax.set_title(f\"Importância das Variáveis - Cenário {nome_cenario.capitalize()} (RF)\")\n",
    "    plt.xlim(0, .7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{base_output}/Imagens/Metricas/importancia_variaveis_rf_{nome_cenario}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Importância das variáveis (Logistic Regression via coeficientes absolutos)\n",
    "    coef = model_lr.coef_[0]\n",
    "    importances_lr = pd.Series(np.abs(coef), index=[legendas_dict.get(v.replace('_bin', ''), v) for v in X.columns])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    importances_lr.sort_values().plot(kind='barh', ax=ax)\n",
    "    ax.set_title(f\"Importância das Variáveis - Cenário {nome_cenario.capitalize()} (LR)\")\n",
    "    plt.xlim(0, 3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{base_output}/Imagens/Metricas/importancia_variaveis_lr_{nome_cenario}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analise dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomes_modelos_legiveis = {\n",
    "    'RandomForest': 'Random Forest',\n",
    "    'LogisticRegression': 'Regressão Logística'\n",
    "}\n",
    "\n",
    "df_metricas = pd.read_csv(f'{base_output}/Tables/metricas_modelos_por_cenario.csv')\n",
    "df_metricas['modelo_legivel'] = df_metricas['modelo'].map(nomes_modelos_legiveis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classe</th>\n",
       "      <th>metric</th>\n",
       "      <th>cenario</th>\n",
       "      <th>modelo</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>f1-score</td>\n",
       "      <td>arrojado</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.882847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>precision</td>\n",
       "      <td>conservador</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.926082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>recall</td>\n",
       "      <td>arrojado</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.989247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>f1-score</td>\n",
       "      <td>moderado</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>precision</td>\n",
       "      <td>arrojado</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>recall</td>\n",
       "      <td>conservador</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.799427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   classe     metric      cenario              modelo     value\n",
       "0       0   f1-score     arrojado        RandomForest  0.882847\n",
       "1       0  precision  conservador        RandomForest  0.926082\n",
       "2       0     recall     arrojado  LogisticRegression  0.989247\n",
       "3       1   f1-score     moderado  LogisticRegression  0.666667\n",
       "4       1  precision     arrojado  LogisticRegression  0.850000\n",
       "5       1     recall  conservador        RandomForest  0.799427"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top_scores = (\n",
    "    df_metricas.sort_values('value', ascending=False)\n",
    "    .groupby(['classe', 'metric'])\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_top_scores = df_top_scores[['classe', 'metric', 'cenario', 'modelo', 'value']]\n",
    "df_top_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metricas_modelos(df, titulo, output_path, filtro_classe=None, col=None, row=None, legenda_fora=True):\n",
    "    if filtro_classe is not None:\n",
    "        df = df[df['classe'] == filtro_classe]\n",
    "\n",
    "    modelos_legiveis = {\n",
    "        'RandomForest': 'Random Forest',\n",
    "        'LogisticRegression': 'Regressão Logística'\n",
    "    }\n",
    "    df = df.copy()\n",
    "    df['modelo_legivel'] = df['modelo'].map(modelos_legiveis)\n",
    "\n",
    "    g = sns.catplot(\n",
    "        data=df,\n",
    "        x='metric',\n",
    "        y='value',\n",
    "        hue='modelo_legivel',\n",
    "        col=col,\n",
    "        row=row,\n",
    "        kind='bar',\n",
    "        palette='Set2',\n",
    "        height=4,\n",
    "        aspect=1.1,\n",
    "        sharey=True\n",
    "    )\n",
    "\n",
    "    g.set_titles('Cenário: {col_name}' if col else '')\n",
    "    if row:\n",
    "        g.set_titles('Cenário: {col_name} | Classe: {row_name}')\n",
    "\n",
    "    g.set_axis_labels('Métrica', 'Desempenho do Modelo')\n",
    "    g.set(ylim=(0, 1))\n",
    "\n",
    "    if g._legend:\n",
    "        g._legend.set_title(None)\n",
    "        if legenda_fora:\n",
    "            g._legend.set_bbox_to_anchor((.86, 0.5))\n",
    "            g._legend.set_loc('center left')\n",
    "\n",
    "    for ax in g.axes.flat:\n",
    "        for label in ax.get_xticklabels():\n",
    "            label.set_rotation(0)\n",
    "\n",
    "    plt.suptitle(titulo, y=1.03, fontsize=14)\n",
    "    g.tight_layout()\n",
    "    g.savefig(output_path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega e prepara o DataFrame\n",
    "df_metricas = pd.read_csv(f'{base_output}/Tables/metricas_modelos_por_cenario.csv')\n",
    "\n",
    "# 1. Comparação geral (sem filtro)\n",
    "plot_metricas_modelos(\n",
    "    df=df_metricas,\n",
    "    titulo='Desempenho Geral dos Modelos - Previsão de Paradas e Não-Paradas',\n",
    "    output_path=f'{base_output}/Imagens/Modelos/comparacao_geral_modelos.png'\n",
    ")\n",
    "\n",
    "# 2. Comparação por cenário - Classe: Parada\n",
    "plot_metricas_modelos(\n",
    "    df=df_metricas,\n",
    "    titulo='Comparação por Cenário - Classe: Parada',\n",
    "    filtro_classe=1,\n",
    "    col='cenario',\n",
    "    output_path=f'{base_output}/Imagens/Modelos/comparacao_por_cenario_parada.png'\n",
    ")\n",
    "\n",
    "# 3. Comparação por cenário - Classe: Não-Parada\n",
    "plot_metricas_modelos(\n",
    "    df=df_metricas,\n",
    "    titulo='Comparação por Cenário - Classe: Não-Parada',\n",
    "    filtro_classe=0,\n",
    "    col='cenario',\n",
    "    output_path=f'{base_output}/Imagens/Modelos/comparacao_por_cenario_nao_parada.png'\n",
    ")\n",
    "\n",
    "# 4. Comparação por cenário e por classe\n",
    "plot_metricas_modelos(\n",
    "    df=df_metricas,\n",
    "    titulo='Comparação por Cenário e Classe',\n",
    "    col='cenario',\n",
    "    row='classe',\n",
    "    output_path=f'{base_output}/Imagens/Modelos/comparacao_por_cenario_e_classe.png'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
